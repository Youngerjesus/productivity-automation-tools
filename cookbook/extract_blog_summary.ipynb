{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Advanced RAG series: IndexingLatest and GreatestLoginSubscribe00Latest and GreatestPostsAdvanced RAG series: IndexingAdvanced RAG series: IndexingHow to optimize embeddings for accurate retrievalDivyanshu Dixit March 01, 2024  Now that we have done the hard work of translating, routing and constructing the query, how do we make sure the response we get back is not inaccurate, or in a worse case entirely made up ie hallucinated. This is where we now get into setting up the data for retrieval and generation. Indexing: We talked about speaking the language of the database when we were doing query construction earlier. Indexing is where we do something similar for the data being queried. There are various ways of doing this, but the end goal is to make it easily understandable for the LLM without losing context. The answer to the question could be anywhere in the document and given the shortcomings of LLMs around real time data, context window and ‚Äúlost in the middle‚Äù problem, it is important to efficiently chunk and then add context to it.  It makes sense to define embeddings at this point, since how we chunk and embed is a core part of indexing to enable accurate retrieval. Put simply, embeddings take a large, complex set of data and convert it into a set of numbers which capture the essence of the data being embedded. This enables the conversion of user query into an embedding (ie a set of numbers) and then retrieving the information based on semantic similarity. How does it look in a multidimensional space? Something like this (note how similar words are closer to each other in this space): Source: Qiming Bao IEEE Back to chunking! For easier understanding, let‚Äôs assume you are working with a large document, such as an e-book. You may want to ask questions about the content in the book, but since it is too large for the LLM‚Äôs context window (and its other limitations), we will like to chunk it up and feed only the portions relevant to the user query to the LLM. But when we do chunk it up, we don‚Äôt want the chunk to lose the context of the story or its characters, imagine just picking up a book and starting to read from page 145 ‚Äì how lost for context will you be? This is where indexing comes in.  Here are a few ways to think about indexing: 1. Chunk Optimization: The first thing to consider here is the data itself, is it short or long? This will define the chunking strategy and the model to use. For example, if you are embedding a sentence, a sentence transformer may suffice, bur for larger docs, you may need to chunk by tokens, so text-embedding-ada-002 may be the model to rely on.  The second thing to consider is the final use case for these embeddings. Are you creating a Q&A chatbot? A summarization tool? Are you using it as an agentic workflow with the output being fed into another LLM for further processing? If it is the latter, you may want to limit the output to the context length of the next LLM for example. Source: Medium @thallyscostalat So, let‚Äôs delve into the various strategies: (i)¬†Rule Based:  Using separators to chunk text, for example space characters, punctuation etc. Here are a few examples of such methods a.¬†Fixed length: The most straightforward way is to chunk based on fixed lengths by counting the number of characters. The mitigant to the risk of losing context by just splitting based on a count, is to have overlapping pieces of text from each chunk, which can be user defined. This is not ideal for obvious reasons, imagine trying to complete a sentence based on the just first half of it. Langchain‚Äôs CharacterTextSplitter is a decent way to test this.¬†Source: FullStackRetrieval Then there are what are called Structure aware splitters ie based on sentence, paragraphs et al.¬† b.¬†NLTK Sentence Tokenizer: This is useful for splitting the given text into sentences. While quite straightforward, it once again has limitations on semantic understanding of underlying text. While great for initial testing, it definitely is not ideal in cases where a context may spawn multiple sentences or paragraphs(which is most of the text we want o query using an LLM). Source: Youtubec.¬†Spacy Sentence Splitter: Another splitter chunking based on sentences and useful when we want to reference smaller chunks. But, suffers from similar disadvantages as NLTK. Source: ashutoshtripathi.com(ii)¬†Recursive structure aware splitting: Combine the Fixed size and Structure aware and we get Recursive structure aware splitting. You will find this extensively used in Langchain docs. The benefit is to be able to control context better. While the chunk sizes may now no more be equal it does help with semantic search, although still not great for structured data. Source: FullStackRetrieval(iii) Content-Aware Splitting:  The previous strategies could be good enough for unstructured data, but when it comes to structured data, it is important to split based on the type of structure itself. That is why there are text splitters specifically for Markdown, LaTeX, HTML, pdf with tables, multimodal ie text+images etc).¬† Source: KDB.AI2.¬†Multi-representation indexing:  Instead of chunking the entire document and retrieving the top-k results based on semantic similarity, what if we could convert the document into compact retrieval units? For example, summary. There are a few ways worth mentioning here: (i)¬†Parent Document:  In this case, based on the user query, one can retrieve the most related chunk and instead of just passing that chunk to the LLM, pass the parent document that the chunk is a part of. This helps improve the context and hence retrieval. However, what if the parent document is bigger than the context window of the LLM? We can make bigger chunks along with the smaller chunks and pass those instead of the parent to fit the context window. Source: clusteredbytes(ii)¬†Dense X Retrieval: Dense Retrieval is a new method for contextual retrieval whereby the chunks are not sentences or paragraphs as we saw earlier. Rather, the authors in the below paper introduce something called ‚Äúproposition‚Äù. A proposition effectively encapsulates the following: -¬†Distinct meaning in the text. The meaning should be captured such that putting all propositions together covers the entire text in terms of semantics -¬†Minimal, ie cannot be further split into smaller propositions -¬†‚Äúcontextualized and self contained‚Äù, meaning each proposition by itself should include all necessary context from the text. Source: ArxivSource: Arxiv The results? Proposition-level retrieval outperforms sentence and passage-level retrieval by 35% and 22.5% respectively (significant improvement). 3.¬†Specialized Embeddings: Domain specific and/or advanced embedding models. (i) Fine-tuning:  Finetuning an embedding model can be quite useful in improving our RAG pipeline‚Äôs ability to retrieve relevant documents. Here, we use the LLM generated queries, the text corpus and the cross reference mapping between the two. This helps the embedding model to understand which corpus to look for. Finetuning embeddings have shown to improve performance anywhere from 5-10%.  Good words of advice here from Jerry Liu on what to keep in mind when finetuning an embedding model: This is most comprehensive tutorial I've seen on embedding fine-tuning for at different stages of LLM/RAG development \\uf8ffüìñ: 1Ô∏è‚É£ When just starting your project, fine-tune the base model before embedding any documents. 2Ô∏è‚É£ As your doc distribution changes in production, fine-tune a‚Ä¶ twitter.com/i/web/status/1‚Ä¶ — Jerry Liu (@jerryjliu0)  Sep 14, 2023 (ii)¬†ColBERT:  This is a retrieval model which enables scalable BERT-based search over large collections of data (in milliseconds). Fast and accurate retrieval is key here. In case you are wondering, BERT stands for Bidirectional Encoder Representations from Transformers. ColBERT stands for Contextual Late Interaction over BERT.  It encodes each passage into a matrix of token-level embeddings (shown below in blue). When a search is performed, it encodes the user query into another matrix of token-level embeddings (in green below). Then it matches query to passages based on context, using ‚Äúscalable vector-similarity (MaxSim) operators.‚Äù¬†¬†Source: Github - Stanford The late-stage interaction is key here to enable fast and scalable retrieval. While cross-encoders evaluate every possible pair of query and documents, leading to high accuracy, this very feature becomes a bug for large scale apps as computational costs rack up. For quick retrieval from large datasets it becomes efficient to precompute document embeddings,¬†leading to a good compromise between compute and quality.¬† 4.¬†Hierarchical Indexing:RAPTOR:  The RAPTOR model as proposed by Stanford researchers is based on tree of document summarization at various abstraction levels ie creating a tree by summarizing clusters of text chunks for more accurate retrieval. The text summarization for retrieval augmentation captures a much larger context across different scales encompassing both thematic comprehension and granularity.  The paper claims significant performance gains by using this method of retrieving with recursive summaries. For example, ‚ÄúOn question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy.‚Äù I will take the 20% improvement in accuracy any day! Source: Arxiv Vector stores which are used to house these embeddings are increasingly becoming commoditized and hence we shall not spend too much time, except that for larger datasets, it does make sense to use scalable solutions. A few players include Pinecone, Weaviate, Chroma (open source) etc.  Having spent the time and effort in Indexing, we shall now see the fruits of this labour in our next section ‚Äì Retrieval! Join the conversationAdd your commentLoginLogin or Subscribe to participate.Latest and GreatestHomePosts¬© 2024 Latest and Greatest.Privacy PolicyTerms of UsePowered by beehiiv\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(None)  # This will disable the caching globally\n",
    "\n",
    "metadata = { \n",
    "    \"model\": \"GPT-3.5\"\n",
    "}\n",
    "\n",
    "loader = WebBaseLoader(\"https://div.beehiiv.com/p/advanced-rag-series-indexing\")\n",
    "title = \"Advanced RAG series: Indexing\"\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "doc = documents[0]\n",
    "\n",
    "doc.page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "class Statement(BaseModel):\n",
    "    statements: List[str] = Field(description=\"글에서 말하고자 하는 목적을 나타내는 문장들 입니다.\")\n",
    "\n",
    "statement_json_parser = JsonOutputParser(pydantic_object=Statement)\n",
    "\n",
    "statement_prompt = PromptTemplate(\n",
    "    input_variables=[\"title\", \"document\"],\n",
    "    partial_variables={\"format_instructions\": statement_json_parser.get_format_instructions()},\n",
    "    template=\"\"\"\n",
    "    너는 글에서 핵심 논지를 추출하는 일을 해야해.\n",
    "    다음 Input 의 Document 은 크롤링 한 문서에 대한 내용이고, Title 은 문서에 대한 제목이야. \n",
    "    문서를 읽고 핵심 논지를 추천해서 알려줘. \n",
    "\n",
    "    Input:\n",
    "    Title: {title}\n",
    "    Document: {document}\n",
    "\n",
    "    출력은 JSON 형식으로 해 주세요. JSON 구조는 다음과 같습니다:\n",
    "    {format_instructions}\n",
    "\n",
    "    Example:\n",
    "    1. Document: Rewrite-Retrieve-Read: This approach focuses on the query input by the user (ie rewriting it), rather than just adapting retriever or the reader. It prompts an LLM to generate a query and then uses a web search engine to retrieve context. There is further alignment via a trainable scheme for the pipeline using a small language model. The summary is well captured in the graphic below:\n",
    "    1. Statement: 이 기법은 사용자로부터 쿼리가 주어졌을 때 해당 쿼리를 재작성하고, 검색 엔진을 통해서 검색해보고, 문맥을 가져와서 답변을 작성하는 방법임. Vector Store 에 검색을 하는 방법은 아니긴 하나, 아이디어는 차용할 수 있다.\n",
    "\n",
    "    2. Document: RAG Fusion: Combination of RAG and reciprocal rank fusion (RRF), by generating multiple queries (to add context from different perspectives), reranking them with reciprocal scores and then fusing the documents and scores. This leads to more comprehensive and accurate answers\n",
    "    2. Statement: 사용자의 질문을 다양한 관점에서 다시 작성해서 여러 쿼리를 만들고 이를 이용해서 문서를 검색한다. 그리고 총 조회된 문서들을 reciprocal rank fusion (RRF) 에 따라서 스코어링을 해서 랭킹을 매긴다. 그러니까 1등 문서는 1/1 점, 2등 문서는 1/2 점, 3등 문서는 1/3 점 이런식으로 점수를 매겨서 총 랭킹을 매기는거임.\n",
    "\n",
    "    3. Document: Step-Back Prompting: This is a more technical prompting technique whereby the LLM does abstractions to derive high level concepts and first principles. This is an iterative process where the user question is used to generate a step back question. The step back answer is then used for reasoning to generate the final answer.\n",
    "    3. Statement: 주어진 질문을 한단계 추상화해서 그 질문으로 답변을 작성하는 방법임.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'statements': ['Indexing is crucial for accurate retrieval by chunking and embedding data for semantic similarity.',\n",
       "  'Different strategies for chunk optimization include rule-based, recursive structure aware splitting, and content-aware splitting.',\n",
       "  'Multi-representation indexing involves converting documents into compact retrieval units like summaries.',\n",
       "  'Specialized embeddings, such as fine-tuning and ColBERT, can improve retrieval performance.',\n",
       "  'Hierarchical indexing with RAPTOR model for document summarization at various abstraction levels leads to significant performance gains.']}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.1, max_tokens=1024)\n",
    "\n",
    "statement_chain = statement_prompt | llm | statement_json_parser\n",
    "\n",
    "statements_result = statement_chain.invoke({\n",
    "    \"title\": title,\n",
    "    \"document\": doc.page_content\n",
    "})\n",
    "\n",
    "statements_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing is crucial for accurate retrieval by chunking and embedding data for semantic similarity.\n",
      "\n",
      "\n",
      "Different strategies for chunk optimization include rule-based, recursive structure aware splitting, and content-aware splitting.\n",
      "\n",
      "\n",
      "Multi-representation indexing involves converting documents into compact retrieval units like summaries.\n",
      "\n",
      "\n",
      "Specialized embeddings, such as fine-tuning and ColBERT, can improve retrieval performance.\n",
      "\n",
      "\n",
      "Hierarchical indexing with RAPTOR model for document summarization at various abstraction levels leads to significant performance gains.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statements = statements_result[\"statements\"]\n",
    "\n",
    "for statement in statements_result['statements']:\n",
    "    print(statement)\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summary(BaseModel): \n",
    "    summary: List[str] = Field(description=\"글을 읽고나서 핵심 내용들을 요약한 글들 입니다. 중요한 내용들을 놓치지 말고 정리해 주세요.\")\n",
    "\n",
    "summary_json_output_parser = JsonOutputParser(pydantic_object=Summary)\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"document\", \"title\", \"statements\"],\n",
    "    partial_variables={\"format_instructions\": summary_json_output_parser.get_format_instructions()},\n",
    "    template=\"\"\"\n",
    "    \n",
    "    당신은 고급 언어 모델(LLM)로서 주어진 텍스트를 읽고, 그 텍스트의 핵심 내용을 추출하고, 추론하여 요약하는 역할을 맡고 있습니다. \n",
    "    주어진 문서의 내용, 제목, 그리고 핵심 논지들을 바탕으로 명확하고 간결한 요약을 작성해 주세요.:\n",
    "\n",
    "    Input:\n",
    "    문서 제목: {title}\n",
    "    문서 내용: {document}\n",
    "    핵심 논지: {statements}\n",
    "\n",
    "    작업 지침:\n",
    "\n",
    "\t1.\t문서의 주요 내용을 정확히 반영하여 요약합니다.\n",
    "\t2.\t문서의 전체적인 맥락과 논지를 유지합니다.\n",
    "\t3.\t불필요한 세부사항은 생략하고, 중요한 정보만 포함합니다.\n",
    "\t4.\t요약은 최대 5-10 문장으로 작성합니다.\n",
    "\n",
    "    출력은 JSON 형식으로 해 주세요. JSON 구조는 다음과 같습니다:\n",
    "    {format_instructions}    \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': ['Indexing is crucial for accurate retrieval by chunking and embedding data for semantic similarity.',\n",
       "  'Different strategies for chunk optimization include rule-based, recursive structure aware splitting, and content-aware splitting.',\n",
       "  'Multi-representation indexing involves converting documents into compact retrieval units like summaries.',\n",
       "  'Specialized embeddings, such as fine-tuning and ColBERT, can improve retrieval performance.',\n",
       "  'Hierarchical indexing with RAPTOR model for document summarization at various abstraction levels leads to significant performance gains.']}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "summary_chain = summary_prompt | llm | summary_json_output_parser\n",
    "\n",
    "summary_result = summary_chain.invoke({\"document\": doc.page_content, \"title\": title, \"statements\": statements})\n",
    "\n",
    "summary_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing is crucial for accurate retrieval by chunking and embedding data for semantic similarity.\n",
      "\n",
      "\n",
      "Different strategies for chunk optimization include rule-based, recursive structure aware splitting, and content-aware splitting.\n",
      "\n",
      "\n",
      "Multi-representation indexing involves converting documents into compact retrieval units like summaries.\n",
      "\n",
      "\n",
      "Specialized embeddings, such as fine-tuning and ColBERT, can improve retrieval performance.\n",
      "\n",
      "\n",
      "Hierarchical indexing with RAPTOR model for document summarization at various abstraction levels leads to significant performance gains.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for summary in summary_result['summary']:\n",
    "    print(summary)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 글의 논지에서 그렇게 주장한 이유를 찾아내야함. \n",
    "# Statement Chain -> Summary Chain\n",
    "# Statement Chain -> Reason Chain  -> formatting_chain \n",
    "\n",
    "class OneStatement(BaseModel):\n",
    "    statement: str = Field(description=\"문서에서 주장한 내용을 나타내는 문장입니다.\")\n",
    "\n",
    "class Reason(BaseModel):\n",
    "    reason: str = Field(description=\"글에서 주장한 내용에 대한 이유를 나타내는 문장입니다.\")\n",
    "\n",
    "class ReasonStatementPair(BaseModel):\n",
    "    statement: OneStatement = Field(description=\"문서에서 주장한 내용을 나타내는 문장입니다.\")\n",
    "    reason: Reason = Field(description=\"글에서 주장한 내용에 대한 이유를 나타내는 문장입니다.\")\n",
    "\n",
    "class ReasonStatementPairList(BaseModel):\n",
    "    reaseon_statement_pair_list: List[ReasonStatementPair] = Field(description=\"문서에서 주장한 내용과 그에 대한 이유를 나타내는 문장들입니다.\")\n",
    "\n",
    "reason_json_output_parser = JsonOutputParser(pydantic_object=ReasonStatementPairList)\n",
    "\n",
    "reason_prompt = PromptTemplate(\n",
    "    input_variables=[\"document\", \"title\", \"statements\"],\n",
    "    partial_variables={\"format_instructions\": reason_json_output_parser.get_format_instructions()},\n",
    "    template=\"\"\"\n",
    "    \n",
    "    당신은 고급 언어 모델(LLM)로서 문서의 핵심 논지들로 부터 왜 이렇게 주장하는지 이유를 추론하는 일을 해야합니다. \n",
    "    핵심 논지들은 여러개가 있으니 각 논지들마다 이유를 추론해야합니다. \n",
    "    \n",
    "    Input:\n",
    "    문서 제목: {title}\n",
    "    문서 내용: {document}\n",
    "    핵심 논지: {statements}\n",
    "\n",
    "    작업 지침:\n",
    "\n",
    "    출력은 JSON 형식으로 해 주세요. JSON 구조는 다음과 같습니다:\n",
    "    {format_instructions}    \n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reaseon_statement_pair_list': [{'statement': {'statement': 'Indexing is crucial for accurate retrieval by chunking and embedding data for semantic similarity.'},\n",
       "   'reason': {'reason': 'Efficiently chunking and adding context to data through indexing enables the conversion of user queries into embeddings for retrieval based on semantic similarity.'}},\n",
       "  {'statement': {'statement': 'Different strategies for chunk optimization include rule-based, recursive structure aware splitting, and content-aware splitting.'},\n",
       "   'reason': {'reason': 'Various chunking strategies like rule-based, recursive structure aware splitting, and content-aware splitting are employed to optimize the data chunking process based on the type of data and the desired retrieval outcomes.'}},\n",
       "  {'statement': {'statement': 'Multi-representation indexing involves converting documents into compact retrieval units like summaries.'},\n",
       "   'reason': {'reason': 'By converting documents into compact retrieval units like summaries, multi-representation indexing enhances the efficiency of retrieval processes by providing a more concise representation of the document content.'}},\n",
       "  {'statement': {'statement': 'Specialized embeddings, such as fine-tuning and ColBERT, can improve retrieval performance.'},\n",
       "   'reason': {'reason': 'Utilizing specialized embeddings like fine-tuning and ColBERT can enhance the retrieval performance by fine-tuning the embedding models to better understand the text corpus and improve relevance in retrieval.'}},\n",
       "  {'statement': {'statement': 'Hierarchical indexing with RAPTOR model for document summarization at various abstraction levels leads to significant performance gains.'},\n",
       "   'reason': {'reason': 'Implementing hierarchical indexing with the RAPTOR model for document summarization at different abstraction levels results in substantial performance improvements by capturing a broader context and enhancing retrieval accuracy.'}}]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reason_chain = reason_prompt | llm | reason_json_output_parser\n",
    "\n",
    "reason_result = reason_chain.invoke({\"document\": doc.page_content, \"title\": title, \"statements\": statements})\n",
    "\n",
    "reason_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement: {'statement': 'Indexing is crucial for accurate retrieval by chunking and embedding data for semantic similarity.'}\n",
      "\n",
      "\n",
      "Reason: {'reason': 'Efficiently chunking and adding context to data through indexing enables the conversion of user queries into embeddings for retrieval based on semantic similarity.'}\n",
      "Statement: {'statement': 'Different strategies for chunk optimization include rule-based, recursive structure aware splitting, and content-aware splitting.'}\n",
      "\n",
      "\n",
      "Reason: {'reason': 'Various chunking strategies like rule-based, recursive structure aware splitting, and content-aware splitting are employed to optimize the data chunking process based on the type of data and the desired retrieval outcomes.'}\n",
      "Statement: {'statement': 'Multi-representation indexing involves converting documents into compact retrieval units like summaries.'}\n",
      "\n",
      "\n",
      "Reason: {'reason': 'By converting documents into compact retrieval units like summaries, multi-representation indexing enhances the efficiency of retrieval processes by providing a more concise representation of the document content.'}\n",
      "Statement: {'statement': 'Specialized embeddings, such as fine-tuning and ColBERT, can improve retrieval performance.'}\n",
      "\n",
      "\n",
      "Reason: {'reason': 'Utilizing specialized embeddings like fine-tuning and ColBERT can enhance the retrieval performance by fine-tuning the embedding models to better understand the text corpus and improve relevance in retrieval.'}\n",
      "Statement: {'statement': 'Hierarchical indexing with RAPTOR model for document summarization at various abstraction levels leads to significant performance gains.'}\n",
      "\n",
      "\n",
      "Reason: {'reason': 'Implementing hierarchical indexing with the RAPTOR model for document summarization at different abstraction levels results in substantial performance improvements by capturing a broader context and enhancing retrieval accuracy.'}\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for pair in reason_result['reaseon_statement_pair_list']:\n",
    "    print(f\"Statement: {pair['statement']}\")\n",
    "    print(\"\\n\")\n",
    "    print (f\"Reason: {pair['reason']}\")\n",
    "\n",
    "\n",
    "print(len(reason_result['reaseon_statement_pair_list']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatting_prompt = PromptTemplate(\n",
    "    input_variables=[\"title\", \"summaries\", \"reason_statement_pair_list\"],\n",
    "    template=\"\"\"\n",
    "    \n",
    "    당신은 고급 언어 모델(LLM)로서 주어진 정보를 바탕으로 글을 포맷팅하여 작성하는 역할을 맡고 있습니다. 주어진 제목, 요약, 핵심 논지들, 그리고 각각의 논지에 대한 이유를 바탕으로 체계적이고 명확하게 Markdown 형식으로 글을 작성해 주세요. 다음과 같은 정보를 제공합니다:\n",
    "\n",
    "    Input:\n",
    "    1.\t글의 제목:  {title}\n",
    "    2.\t추론한 요약: {summaries}\n",
    "    3.\t핵심 논지들 및 이유들: {reason_statement_pair_list}\n",
    "\n",
    "    작업 지침:\n",
    "\t1.\t제공된 제목, 요약, 논지, 이유를 기반으로 체계적으로 글을 작성합니다.\n",
    "\t2.\t글의 서론에서는 요약을 포함하여 전체적인 내용을 소개합니다.\n",
    "\t3.\t본론에서는 각 핵심 논지를 제시하고, 각각의 논지를 뒷받침하는 이유를 설명합니다.\n",
    "\t4.\t결론에서는 요약과 논지들을 다시 강조하며 글을 마무리합니다.\n",
    "\t5.\t글은 논리적이고 일관되게 작성하며, 각 문단은 명확한 주제를 포함합니다.\n",
    "\t6.\t최종 출력 형식은 Markdown입니다.\n",
    "\n",
    "    예시: \n",
    "    {{\n",
    "        \"글의 제목\": “AI의 발전과 윤리적 문제”,\n",
    "        \"추론한 요약\": \"인공지능은 빠르게 발전하며 의료, 금융, 교육 등 다양한 분야에서 혁신을 주도하고 있다. 그러나 이러한 발전에는 여러 윤리적 문제들이 따르며, 이를 해결하기 위한 연구와 노력이 필요하다\", \n",
    "        \"핵심 논지들 및 이유들:\": [\n",
    "            {{\n",
    "                \"핵심 논지\": \"AI의 발전은 편향성을 초래할 수 있다\", \n",
    "                \"이유\": “AI 시스템이 학습하는 데이터가 편향되어 있을 경우, 그 결과도 편향될 수 있다. 이는 특정 그룹에 불이익을 줄 수 있다.”, \n",
    "            }},\n",
    "            {{\n",
    "                \"핵심 논지\": \"AI는 개인 프라이버시 침해의 위험이 있다\", \n",
    "                \"이유\": \"AI가 수집하고 처리하는 방대한 데이터에는 개인의 민감한 정보가 포함될 수 있으며, 이에 대한 적절한 보호가 필요하다.”, \n",
    "            }},\n",
    "            {{\n",
    "                \"핵심 논지\": \"AI의 결정 과정은 투명성을 요구한다\", \n",
    "                \"이유\": \"AI의 자동화된 결정은 이해하기 어려울 수 있으며, 이는 책임 소재를 명확히 하기 위해 투명한 프로세스가 필요하다”, \n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    작성된 글 (Markdown 형식):\n",
    "    # AI의 발전과 윤리적 문제\n",
    "\n",
    "    ## 핵심 논지와 이유\n",
    "\n",
    "    ### 핵심 논지 1\n",
    "    AI의 발전은 편향성을 초래할 수 있다.\n",
    "\n",
    "    #### 이유\n",
    "    AI 시스템이 학습하는 데이터가 편향되어 있을 경우, 그 결과도 편향될 수 있다. 이는 특정 그룹에 불이익을 줄 수 있다.\n",
    "\n",
    "    ### 핵심 논지 2\n",
    "    AI는 개인 프라이버시 침해의 위험이 있다.\n",
    "\n",
    "    #### 이유\n",
    "    AI가 수집하고 처리하는 방대한 데이터에는 개인의 민감한 정보가 포함될 수 있으며, 이에 대한 적절한 보호가 필요하다.\n",
    "\n",
    "    ### 핵심 논지 3\n",
    "    AI의 결정 과정은 투명성을 요구한다.\n",
    "\n",
    "    #### 이유\n",
    "    AI의 자동화된 결정은 이해하기 어려울 수 있으며, 이는 책임 소재를 명확히 하기 위해 투명한 프로세스가 필요하다.\n",
    "\n",
    "    ## 요약 정보\n",
    "\n",
    "    인공지능(AI)은 빠르게 발전하며 의료, 금융, 교육 등 다양한 분야에서 혁신을 주도하고 있습니다. 이러한 기술 발전은 우리의 생활 방식을 급격히 변화시키고 있으며, 많은 긍정적인 영향을 미치고 있습니다. 그러나, 이러한 발전의 이면에는 중요한 윤리적 문제가 존재합니다.\n",
    "\n",
    "    첫째로, AI의 발전은 편향성을 초래할 수 있습니다. AI 시스템이 학습하는 데이터가 편향되어 있을 경우, 그 결과도 편향될 수 있습니다. 이는 특정 그룹에 불이익을 줄 수 있습니다. 둘째로, AI는 개인 프라이버시 침해의 위험이 있습니다. AI가 수집하고 처리하는 방대한 데이터에는 개인의 민감한 정보가 포함될 수 있으며, 이에 대한 적절한 보호가 필요합니다. 셋째로, AI의 결정 과정은 투명성을 요구합니다. AI의 자동화된 결정은 이해하기 어려울 수 있으며, 이는 책임 소재를 명확히 하기 위해 투명한 프로세스가 필요합니다.\n",
    "\n",
    "    이러한 문제를 해결하기 위해서는 기술적 진보뿐만 아니라 윤리적 기준 마련이 필수적입니다. 연구자들과 정책 입안자들은 AI 기술이 윤리적 원칙을 준수하며 발전할 수 있도록 긴밀히 협력해야 합니다. 구체적인 가이드라인과 규제는 AI 시스템이 신뢰성을 유지하며, 공정하게 사용될 수 있도록 돕습니다.\n",
    "\n",
    "    결론적으로, AI의 발전은 우리의 삶에 많은 혜택을 가져오지만, 그에 따른 윤리적 문제도 간과해서는 안 됩니다. 이를 해결하기 위해서는 지속적인 연구와 노력이 필요하며, 사회 전반에 걸쳐 윤리적 기준을 마련하는 것이 중요합니다. \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Advanced RAG series: Indexing\\n\\n## 핵심 논지와 이유\\n\\n### 핵심 논지 1\\nIndexing is crucial for accurate retrieval by chunking and embedding data for semantic similarity.\\n\\n#### 이유\\nEfficiently chunking and adding context to data through indexing enables the conversion of user queries into embeddings for retrieval based on semantic similarity.\\n\\n### 핵심 논지 2\\nDifferent strategies for chunk optimization include rule-based, recursive structure aware splitting, and content-aware splitting.\\n\\n#### 이유\\nVarious chunking strategies like rule-based, recursive structure aware splitting, and content-aware splitting are employed to optimize the data chunking process based on the type of data and the desired retrieval outcomes.\\n\\n### 핵심 논지 3\\nMulti-representation indexing involves converting documents into compact retrieval units like summaries.\\n\\n#### 이유\\nBy converting documents into compact retrieval units like summaries, multi-representation indexing enhances the efficiency of retrieval processes by providing a more concise representation of the document content.\\n\\n### 핵심 논지 4\\nSpecialized embeddings, such as fine-tuning and ColBERT, can improve retrieval performance.\\n\\n#### 이유\\nUtilizing specialized embeddings like fine-tuning and ColBERT can enhance the retrieval performance by fine-tuning the embedding models to better understand the text corpus and improve relevance in retrieval.\\n\\n### 핵심 논지 5\\nHierarchical indexing with RAPTOR model for document summarization at various abstraction levels leads to significant performance gains.\\n\\n#### 이유\\nImplementing hierarchical indexing with the RAPTOR model for document summarization at different abstraction levels results in substantial performance improvements by capturing a broader context and enhancing retrieval accuracy.\\n\\n## 요약 정보\\n\\nIndexing plays a crucial role in accurate data retrieval by chunking and embedding data for semantic similarity. Different strategies for chunk optimization, such as rule-based, recursive structure aware splitting, and content-aware splitting, are employed to enhance the data chunking process. Multi-representation indexing involves converting documents into compact retrieval units like summaries to improve retrieval efficiency. Specialized embeddings like fine-tuning and ColBERT can enhance retrieval performance by fine-tuning the models. Hierarchical indexing with the RAPTOR model for document summarization at various abstraction levels leads to significant performance gains. These strategies collectively contribute to improving the efficiency and accuracy of data retrieval processes.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "formatting_chain = formatting_prompt | llm | StrOutputParser()\n",
    "\n",
    "formatting_result = formatting_chain.invoke({\"title\": title, \"summaries\": summary_result['summary'], \"reason_statement_pair_list\": reason_result['reaseon_statement_pair_list']})\n",
    "\n",
    "formatting_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file exists. version:v0.0.1\n",
      "Markdown file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def increment_version(version):\n",
    "    major, minor, patch = map(int, version.lstrip('v').split('.'))\n",
    "\n",
    "    patch += 1\n",
    "\n",
    "    if patch == 10:\n",
    "        patch = 0\n",
    "        minor += 1\n",
    "\n",
    "    if minor == 10:\n",
    "        minor = 0\n",
    "        major += 1\n",
    "\n",
    "    new_version = f\"v{major}.{minor}.{patch}\"\n",
    "    return new_version\n",
    "\n",
    "markdown_content = formatting_result\n",
    "\n",
    "default_version = \"v0.0.0\"\n",
    "\n",
    "# Specify the file path and name\n",
    "file_path = f\"files/{title}:{default_version}.md\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    new_version = increment_version(default_version)\n",
    "    file_path = f\"files/{title}:{new_version}.md\"\n",
    "    print(f\"The file exists. version:{new_version}\")\n",
    "\n",
    "else:\n",
    "    print(f\"The file_path: {file_path} does not exist.\")\n",
    "\n",
    "# Write the content to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(markdown_content)\n",
    "\n",
    "print(\"Markdown file saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
